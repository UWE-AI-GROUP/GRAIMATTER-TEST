{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "# Classifiers for attack models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import t=privachy version \n",
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the datasets\n",
    "1. We draw data points from a distribution.\n",
    "2. We split these data points into the target dataset and a shadow dataset drawn from the same distribution.\n",
    "3. We also draw a dataset from a different distribution.\n",
    "\n",
    "**NOTE**. ***I make datasets with few samples but with many features to force the target model to overfit.***\n",
    "\n",
    "\n",
    "***NOTE JIM: had to make batch_size 25 so DP optimizer would run with same hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "\n",
    "# (X,y): Original distribution\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_classes=n_classes, \n",
    "                           n_features=300,\n",
    "                           n_informative=300,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           random_state=15\n",
    "                          )\n",
    "# One-hot encoding of the label\n",
    "y = np.eye(n_classes)[y]\n",
    "\n",
    "# (Xt, yt) is the target dataset, owned by the TRE and drawn from the (X,y) distribution\n",
    "# (Xs, ys) is a shadow dataset drawn from the (X,y) distribution\n",
    "Xt, Xs, yt, ys = train_test_split(X, y, test_size=0.50, random_state=15)\n",
    "\n",
    "# (Xd, yd) is a shadow dataset, drawn from a different distribution (different seed)\n",
    "Xd, yd = make_classification(n_samples=1000,\n",
    "                           n_classes=n_classes, \n",
    "                           n_features=300,\n",
    "                           n_informative=300,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           random_state=42\n",
    "                          )\n",
    "yd = np.eye(n_classes)[yd]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (member) and test (non-member) datasets\n",
    "# Set shuffle to False so that Xt_membership is consistent with Xt, otherwise\n",
    "# we need to stack Xt_member and Xt_nonmember again to get a consistent Xt.\n",
    "Xt_member, Xt_nonmember, yt_member, yt_nonmember = train_test_split(Xt, yt, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Set membership status for future tests\n",
    "Xt_membership = np.vstack(\n",
    "    (\n",
    "        np.ones((Xt_member.shape[0], 1), np.uint8),\n",
    "        np.zeros((Xt_nonmember.shape[0], 1), np.uint8)\n",
    "    )\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the target model architecture\n",
    "\n",
    "*Again, I'm using a rather big model (for the classification task) to favour overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target model\n",
    "# Tensorflow model (MLP) (making it big to make it overfit)\n",
    "\n",
    "input_data = Input(shape = Xt_member[0].shape)\n",
    "x = Dense(128, activation='relu')(input_data)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try the SafeKerasModel version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import safemodel\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=None\n",
    "safeModel = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekeras-test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safeModel.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "safeModel.compile(loss=loss)#optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theType= type(safeModel.optimizer)\n",
    "print(f'optimiser is type {theType}')\n",
    "\n",
    "dpused,reason = safeModel.check_optimizer_is_DP(safeModel.optimizer)\n",
    "print(f' It is {dpused} that the model will be DP because {reason}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 25\n",
    "\n",
    "r_DP = safeModel.fit(Xt_member, \n",
    "              yt_member, \n",
    "              validation_data=(Xt_nonmember, yt_nonmember),\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size\n",
    ")  \n",
    "\n",
    "\n",
    "plt.plot(r_DP.history['accuracy'], label='accuracy')\n",
    "plt.plot(r_DP.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theType= type(safeModel.optimizer)\n",
    "print(f'optimiser is type {theType}')\n",
    "\n",
    "dpused,reason = safeModel.check_DP_used(safeModel.optimizer)\n",
    "print(f' It is {dpused} that the model will be DP because {reason}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dataset has {Xt.shape[0]} entries so batch size is {100*safeModel.batch_size/Xt.shape[0]}%')\n",
    "privacy = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=Xt.shape[0],\n",
    "                                              batch_size=safeModel.batch_size,\n",
    "                                              noise_multiplier=safeModel.noise_multiplier,\n",
    "                                              epochs=25,#epochs,\n",
    "                                              delta=1e-5)\n",
    "print(f'with these settings privacy = {privacy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.save('safekeras.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.preliminary_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in safeModel.__dict__.items():\n",
    "     print (f'thing associated with key {key} has type {type(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = []\n",
    "unusuals =[]\n",
    "tuples =[]\n",
    "deleted_keys = []\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Tuples\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == tuple):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "        tuples.append(key)\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Bools\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == bool):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Lists\")\n",
    "print(\"===============================================================================================\")        \n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == list):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Strings\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == str):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "        \n",
    "print(\"===============================================================================================\")\n",
    "print(\"Ints\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == int):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Floats\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == float):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "        \n",
    "print(\"===============================================================================================\")\n",
    "print(\"Dicts\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == dict):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "        \n",
    "print(\"===============================================================================================\")\n",
    "print(\"Sets\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == set):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Unusual Ones\")\n",
    "print(\"===============================================================================================\")\n",
    "\n",
    "        \n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if((type(value) != int) and (type(value) != str) and \n",
    "       (type(value) != list) and (type(value) != bool) and \n",
    "       (type(value) != tuple) and (type(value) != float) and\n",
    "       (type(value) != dict) and (type(value) != set)):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "        unusuals.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safeModel.request_release('safekeras.pkl') # TypeError: cannot pickle '_thread.RLock' object\n",
    "\n",
    "#safeModel.request_release('safekeras.sav') # TypeError: cannot pickle '_thread.RLock' object\n",
    "\n",
    "#safeModel.request_release('safekeras.tf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "safeModel.save('my_model')\n",
    "safeModel.save_weights('weights.h5')\n",
    "\n",
    "safeModel.request_release('safe.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "safeModel.request_release('safekeras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "optimizer=None\n",
    "safeModel = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekeras-test\")\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "safeModel.compile(loss=loss)#optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 25\n",
    "\n",
    "r_DP = safeModel.fit(Xt_member, \n",
    "              yt_member, \n",
    "              validation_data=(Xt_nonmember, yt_nonmember),\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "## Test 1 - researcher doesn't change recommended params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "\n",
    "# create and fit using recommended params\n",
    "\n",
    "\n",
    "print(\"***Test 1: researcher doesn't change recommended params\")\n",
    "optimizer=None\n",
    "safeKerasModel1 = Safe_KerasModel(inputs=input_data, outputs=output,name=\"safekerasmodel1-test\")\n",
    "safeKerasModel1.compile(loss=loss)\n",
    "safeKerasModel1.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel1.save(\"safe1.h5\", include_optimizer=False, save_format='h5')\n",
    "safeKerasModel1.preliminary_check()\n",
    "safeKerasModel1.request_release(filename=\"safe1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - researcher changes params safely\n",
    "Inputs: \n",
    "\n",
    "l2_norm_clip = 1.1, noise_multiplier=0.6\n",
    "\n",
    "Expected Result:\n",
    "\n",
    "Remain Unchanged l2_norm_clip = 1.1, noise_multiplier=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "# change model params to recommended values\n",
    "print(\"\\n***Test 2: researcher changes params safely\")\n",
    "safeKerasModel2 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel2-test\", l2_norm_clip = 1.1, noise_multiplier=0.6)\n",
    "safeKerasModel2.optimizer=tensorflow_privacy.DPKerasAdamOptimizer\n",
    "safeKerasModel2.compile(loss=loss)\n",
    "safeKerasModel2.l2_norm_clip = 1.1\n",
    "safeKerasModel2.noise_multiplier = 0.6\n",
    "safeKerasModel2.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "\n",
    "safeKerasModel2.save(\"safe2.hd5\")\n",
    "safeKerasModel2.preliminary_check()\n",
    "safeKerasModel2.request_release(filename=\"safe2.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - researcher changes string params unsafely\n",
    "Expected result:\n",
    "\n",
    "WARNING: model parameters may present a disclosure risk:\n",
    "- parameter l2_norm_clip = 0.8 identified as less than the recommended min value of 1.0.\n",
    "Changed parameter l2_norm_clip = 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "# change one model params in an unsafe way\n",
    "print(\"\\n***Test 3: researcher changes params unsafely\")\n",
    "safeKerasModel3 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel3-test\", l2_norm_clip = 0.8)\n",
    "safeKerasModel3.compile(loss=loss)\n",
    "#safeKerasModel3.optimizer=\"Adam\"\n",
    "safeKerasModel3.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel3.save(\"unsafe3.hd5\")\n",
    "safeKerasModel3.preliminary_check()\n",
    "safeKerasModel3.request_release(filename=\"unsafe3.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - researcher changes string params unsafely\n",
    "Input delta = -1 \n",
    "\n",
    "Expected Result:\n",
    "\n",
    "WARNING: model parameters may present a disclosure risk:\n",
    "- parameter delta = -1 identified as less than the recommended min value of 1e-05.\n",
    "Changed parameter delta = 1e-05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change another model params in an  unsafe way\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "print(\"\\n***Test 3: researcher changes string params unsafely\")\n",
    "safeKerasModel4 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel3-test\", delta=-1)\n",
    "safeKerasModel4.compile(loss=loss)\n",
    "#safeKerasModel3.optimizer=\"Adam\"\n",
    "safeKerasModel4.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel4.save(\"unsafe3.hd5\")\n",
    "safeKerasModel4.preliminary_check()\n",
    "safeKerasModel4.request_release(filename=\"unsafe3.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: researcher changes params unsafely\n",
    "input: l2_norm_clip=0.9, noise_multipier=0.4, min_epsilon=9,delta=-1\n",
    "\n",
    "Expected Result:\n",
    "\n",
    "WARNING: model parameters may present a disclosure risk:\n",
    "- parameter l2_norm_clip = 0.9 identified as less than the recommended min value of 1.0.\n",
    "Changed parameter l2_norm_clip = 1.0.\n",
    "- parameter min_epsilon = 9 identified as less than the recommended min value of 10.\n",
    "Changed parameter min_epsilon = 10.\n",
    "- parameter delta = -1 identified as less than the recommended min value of 1e-05.\n",
    "Changed parameter delta = 1e-05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change another model params in an  unsafe way\n",
    "print(\"\\n***Test 5: researcher changes string and numeric params unsafely\")\n",
    "\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "\n",
    "safeKerasModel4 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel3-test\", l2_norm_clip=0.9, noise_multipier=0.4, min_epsilon=9,delta=-1)\n",
    "safeKerasModel4.compile(loss=loss)\n",
    "safeKerasModel4.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel4.save(\"unsafe4.hd5\")\n",
    "safeKerasModel4.preliminary_check()\n",
    "safeKerasModel4.request_release(filename=\"unsafe4.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6 researcher changes params unsafely\n",
    "\n",
    "Expected result:\n",
    "\n",
    "Input optimizer = wobble\n",
    "\n",
    "WARNING: model parameters may present a disclosure risk\n",
    "Unknown optimizer wobble - Changed parameter optimizer = 'DPKerasSGDOptimizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "# change another model params in an  unsafe way but tells preliminary_check() not to overwrite params\n",
    "print(\"\\n***Test 6: researcher changes string and numeric params unsafely\")\n",
    "safeKerasModel6 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel6-test\", optimizer=\"wobble\")\n",
    "safeKerasModel6.compile(loss=loss)\n",
    "safeKerasModel6.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel6.save(\"unsafe6.hd5\")\n",
    "\n",
    "safeKerasModel6.preliminary_check(apply_constraints=False)\n",
    "safeKerasModel6.request_release(filename=\"unsafe6.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7 researcher changes params unsafely\n",
    "\n",
    "Inputs:\n",
    "\n",
    "Input optimizer = Adam\n",
    "\n",
    "Expected Result:\n",
    "\n",
    "WARNING: model parameters may present a disclosure risk\n",
    "Changed parameter optimizer = 'DPKerasAdamOptimizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "# change another model params in an  unsafe way but tells preliminary_check() not to overwrite params\n",
    "print(\"\\n***Test 7: researcher changes string and numeric params unsafely\")\n",
    "safeKerasModel7 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel6-test\", optimizer=\"Adam\")\n",
    "safeKerasModel7.compile(loss=loss)\n",
    "safeKerasModel7.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel7.save(\"unsafe7.hd5\")\n",
    "\n",
    "safeKerasModel7.preliminary_check(apply_constraints=False)\n",
    "safeKerasModel7.request_release(filename=\"unsafe7.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "# change another model params in an  unsafe way but tells preliminary_check() not to overwrite params\n",
    "print(\"\\n***Test 8: researcher changes string and numeric params unsafely\")\n",
    "safeKerasModel8 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel6-test\", optimizer=\"Adam\")\n",
    "safeKerasModel8.compile(loss=loss)\n",
    "safeKerasModel8.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel8.save(\"unsafe8.h5\", include_optimizer=False, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "f = tf.keras.models.load_model('unsafe8.h5', custom_objects={\"Safe_KerasModel\": Safe_KerasModel})\n",
    "kerasmodelkeys =[]\n",
    "for key,value in f.__dict__.items():\n",
    "        #print ( f' key {key} : value {value} \\n')\n",
    "        kerasmodelkeys.append(key)\n",
    "\n",
    "        \n",
    "ourmodelkeys =[]\n",
    "for key,value in safeKerasModel8.__dict__.items():\n",
    "        #print ( f' key {key} : value {value} \\n')\n",
    "        ourmodelkeys.append(key)\n",
    "\n",
    "print(f'Our model has {len(ourmodelkeys)} keys')\n",
    "print(f'The keras model has {len(kerasmodelkeys)} keys')\n",
    "ourmodelkeys.sort()\n",
    "kerasmodelkeys.sort()\n",
    "\n",
    "print(ourmodelkeys)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in safeKerasModel8.__dict__.items():\n",
    "    if 'optimizer' in key:\n",
    "        print ( f' key {key} : value {value} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dp_epsilon_met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "samples = [250, 250, 250]\n",
    "batch_sizes = [1, 5, 25]\n",
    "epochs = [20, 200, 2500]\n",
    "noisemult = [0.7, 0.8, 0.9]\n",
    "\n",
    "okslist = []\n",
    "epsilonslist = []\n",
    "noisemultslist = []\n",
    "sampleslist = []\n",
    "batch_size_list = []\n",
    "epochs_list = []\n",
    "\n",
    "safeKerasModel8 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel6-test\", optimizer=\"Adam\")\n",
    "\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        for k in range(0,3):\n",
    "            sampleslist.append(samples[i])\n",
    "            safeKerasModel8.noise_multiplier = noisemult[i]\n",
    "            noisemultslist.append(noisemult[i])\n",
    "            batch_size_list.append(batch_sizes[j])\n",
    "            epochs_list.append(epochs[k])\n",
    "            ok, epsilon = safeKerasModel8.dp_epsilon_met(num_examples=samples[i],batch_size=batch_sizes[j],epochs=epochs[k])\n",
    "            okslist.append(ok)\n",
    "            epsilonslist.append(epsilon)\n",
    "\n",
    "for i in range(0, len(okslist)):\n",
    "    pass\n",
    "    #print(f'{okslist[i]} {epsilonslist[i]} {noisemultslist[i]} {sampleslist[i]} {epochs_list[i]}' )\n",
    "    \n",
    "\n",
    "    \n",
    "mydf = pd.DataFrame(\n",
    "    {'OK': okslist,\n",
    "     'Epsilon': epsilonslist,\n",
    "     'Noise Multiplier': noisemultslist,\n",
    "     'Num Samples': sampleslist,\n",
    "     'Epochs': epochs_list,\n",
    "     'Batch Size': batch_size_list\n",
    "\n",
    "\n",
    "    })\n",
    "\n",
    "mydf.round(decimals=2).sort_values('Epsilon')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meets DP Epsilon Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "epochs=20\n",
    "batch_size=1\n",
    "# change another model params in an  unsafe way i.e. Optimizer=Adam\n",
    "print(\"\\n***Test 9: Meets DP epsilon criteria\")\n",
    "safeKerasModel9 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel9-test\", optimizer=\"Adam\")\n",
    "safeKerasModel9.compile(loss=loss)\n",
    "safeKerasModel9.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel9.save(\"unsafe7.hd5\")\n",
    "\n",
    "safeKerasModel9.preliminary_check(apply_constraints=False)\n",
    "safeKerasModel9.request_release(filename=\"unsafe9.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
