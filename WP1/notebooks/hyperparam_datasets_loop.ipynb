{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a469e5",
   "metadata": {},
   "source": [
    "# Notebook to loop over datasets, models, scenarios and hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcf9e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data_preprocessing/data_interface.py:ROOT PROJECT FOLDER = /home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "import hashlib\n",
    "import itertools\n",
    "import warnings\n",
    "import logging\n",
    "import pandas as pd\n",
    "from metrics import get_metrics\n",
    "from sklearn import datasets as skl_datasets\n",
    "from scenarios import *\n",
    "from plots import *\n",
    "from experiments import ResultsEntry\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "#print(PROJECT_ROOT)\n",
    "from data_preprocessing.data_interface import get_data_sklearn, DataNotAvailable\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa6ed0",
   "metadata": {},
   "source": [
    "Load the experimental config from the config file. The config file should include a single dictionary with the following keys:\n",
    "1. `datasets`: a list of the dataset names (strings) to use\n",
    "1. `classifiers`: a list of the classifiers to use. Each item is itself a list with two elements, the first being the string of the module (e.g. `\"sklearn.svm\"`) and the second being the class name (e.g. `\"SVC\"`)\n",
    "1. `experiment_params`: A dictionary where each classifier class name from above is a key and the value is another dictionary where the keys are the hyperparameters to vary and the values are lists over the values the hyper-parameter should take. Note in json, use `true, false, null` instead of `True, False, None`.\n",
    "1. `results_filename`: string of a filename in which to save the results. Will overwrite happily.\n",
    "1. `n_reps`: how many times to repeat each combination of dataset, model, scenario, hyper-parameter\n",
    "1. `mia_classifier`: the classifier to use for the attack model. Currently restricted to default hyper-params. Should be specified as a list with two items, as per `classifiers`\n",
    "1. `scenarios`: a list of the scenarios to run. Can only include a subset of \"WorstCase\", \"Salem1\", and \"Salem2\" at the moment\n",
    "\n",
    "An example config file is provided as `example_loop_experiment_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ed8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG_FILENAME = \"example_loop_experiment_config.json\"\n",
    "CONFIG_FILENAME = \"randomForest_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FILENAME, 'r') as f:\n",
    "    config = json.loads(f.read())\n",
    "\n",
    "datasets = config['datasets']\n",
    "classifier_strings = config['classifiers']\n",
    "\n",
    "classifiers = {}\n",
    "for module_name, class_name in classifier_strings:\n",
    "    module = importlib.import_module(module_name)\n",
    "    class_ = getattr(module, class_name)\n",
    "    classifiers[class_name] = class_\n",
    "\n",
    "experiment_params = config['experiment_params']\n",
    "\n",
    "results_filename = config['results_filename']\n",
    "\n",
    "n_reps = config['n_reps']\n",
    "\n",
    "mia_classifier_module, mia_classifier_name = config['mia_classifier']\n",
    "module = importlib.import_module(mia_classifier_module)\n",
    "mia_classifier = getattr(module, mia_classifier_name)\n",
    "\n",
    "scenarios = config['scenarios']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81ad45",
   "metadata": {},
   "source": [
    "@Alba: didn't remove this in case you wanted to keep all those hyper-param lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b3cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_params = {\n",
    "#     'RandomForestClassifier': {\n",
    "#         #'n_estimators': [10, 20, 100],\n",
    "#         #'criterion':['gini','entropy'],\n",
    "#         #'max_depth':[None,2,4],\n",
    "#         #'max_features':[None,'sqrt','log2'],\n",
    "#         'bootstrap': [True, False],\n",
    "#         'min_samples_split': [2, 10],\n",
    "#         #'class_weight':[None,'balanced','balanced_subsample'],\n",
    "#     },\n",
    "#     'DecisionTreeClassifier': {\n",
    "#         #'criterion':['gini','entropy'],\n",
    "#         'max_depth':[None,2,4],\n",
    "#         #'min_samples_split': [2, 10],\n",
    "#         #'max_features':[None,'sqrt','log2'],\n",
    "#         #'class_weight':[None,'balanced']\n",
    "#     },\n",
    "#     'GaussianProcessClassifier': {\n",
    "#         'max_iter_predict':[50,100,200],\n",
    "#         'warm_start':[True,False],\n",
    "#     },\n",
    "#     'MLPClassifier': {\n",
    "#         #'hidden_layer_size':[(50,),(100,),(200,)],\n",
    "#         #'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "#         'solver':['lbfgs', 'sgd', 'adam'],\n",
    "#         #'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "#         #'max_iter': [50,200,400,1000]\n",
    "#     },\n",
    "#     'KNeighborsClassifier': {\n",
    "#         'n_neighbors':[2,5,10,20],\n",
    "#         'weights':['uniform', 'distance'],\n",
    "#         #'algorithm':['ball_tree', 'kd_tree', 'brute']\n",
    "#     },\n",
    "#     'SVC': {\n",
    "#         #'Kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#         #'decision_function_shape':['ovo', 'ovr'],\n",
    "#         #'max_iter':[-1, 2, 5],\n",
    "#         'probability':[True]\n",
    "#     },\n",
    "#     'AdaBoostClassifier': {\n",
    "#         #'n_estimators': [10, 20, 50, 100],\n",
    "#         'algorithm':['SAMME', 'SAMME.R']\n",
    "#     }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "relevant-tours",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data_preprocessing/data_interface.py:DATASET FOLDER = /home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data\n",
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-hospital-mortality\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8936466297a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"WorstCase\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscenarios\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"WorstCase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     mi_test_x, mi_test_y, mi_clf = worst_case_mia(\n\u001b[0m\u001b[1;32m     48\u001b[0m                         \u001b[0mtarget_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0mX_target_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/WP1/notebooks/scenarios.py\u001b[0m in \u001b[0;36mworst_case_mia\u001b[0;34m(target_model, X_target_train, X_test, prop_mia_train, mia_classifier)\u001b[0m\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mmia_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_train_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmia_test_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/WP1/notebooks/scenarios.py\u001b[0m in \u001b[0;36mtrain_mia\u001b[0;34m(mia_train_probs, mia_train_labels, mia_classifier)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mTrain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmia\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mmia_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_train_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmia_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         indices = _generate_sample_indices(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_generate_sample_indices\u001b[0;34m(random_state, n_samples, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    126\u001b[0m     Private function used to _parallel_build_trees function.\"\"\"\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mrandom_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0msample_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_random_state\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_mt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mbit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random.bit_generator.BitGenerator.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"once\")\n",
    "    #MPLClassifier is giving a lot of warnings. \n",
    "    # For each repetition are the same, so it will only show the same warning once.\n",
    "\n",
    "for dataset in datasets[1:3]:\n",
    "    #load the data\n",
    "    try:\n",
    "        X, y = get_data_sklearn(dataset)\n",
    "    except DataNotAvailable as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    print(dataset)\n",
    "    for r in range(n_reps):\n",
    "        #split into training, shadow model and validation data\n",
    "        X_target_train, X_shadow_train, X_test, y_target_train, y_shadow_train, y_test = split_target_data(X.values, y.values.flatten(), r_state=r)\n",
    "        \n",
    "        for classifier_name, clf_class in classifiers.items():\n",
    "            all_combinations = itertools.product(*experiment_params[classifier_name].values())\n",
    "            for i,combination in enumerate(all_combinations):\n",
    "\n",
    "                # Turn this particular combination into a dictionary\n",
    "                params = {n: v for n, v in zip(experiment_params[classifier_name].keys(), combination)}\n",
    "                target_classifier = clf_class()\n",
    "                target_classifier.set_params(**params)\n",
    "\n",
    "                # Train the target model\n",
    "                target_classifier.fit(X_target_train, y_target_train.ravel())#convert that array shape to (n, ) (i.e. flatten it) -- Fix warning\n",
    "\n",
    "                # Get target metrics\n",
    "                target_metrics = {f\"target_{key}\": val for key, val in get_metrics(target_classifier, X_test, y_test).items()}\n",
    "                target_train_metrics = {f\"target_train_{key}\": val for key, val in get_metrics(target_classifier, X_target_train, y_target_train).items()}\n",
    "                target_metrics = {**target_metrics, **target_train_metrics}\n",
    "               \n",
    "                hashstr = f'{dataset} {classifier_name} {str(params)}'\n",
    "                model_data_param_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "                \n",
    "                hashstr = f'{str(params)}'\n",
    "                param_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "                \n",
    "                ##########################################\n",
    "                #######   Worst case scenario     ########\n",
    "                ##########################################\n",
    "                if \"WorstCase\" in scenarios:\n",
    "                    scenario = \"WorstCase\"\n",
    "                    mi_test_x, mi_test_y, mi_clf = worst_case_mia(\n",
    "                        target_classifier,\n",
    "                        X_target_train,\n",
    "                        X_test,\n",
    "                        mia_classifier=mia_classifier()\n",
    "                    )\n",
    "                    # Get MIA metrics\n",
    "                    mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "\n",
    "                    #Create ID for dataset classifier parameters scenario (but not repetition/random split)\n",
    "                    hashstr = f'{dataset} {classifier_name} {str(params)} {scenario}'\n",
    "                    full_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    new_results = ResultsEntry(\n",
    "                        full_id, model_data_param_id, param_id,\n",
    "                        dataset,\n",
    "                        scenario,\n",
    "                        classifier_name,\n",
    "                        attack_classifier_name=mia_classifier_name,\n",
    "                        repetition=r,\n",
    "                        params=params,\n",
    "                        target_metrics=target_metrics,\n",
    "                        mia_metrics=mia_metrics\n",
    "                    )\n",
    "\n",
    "                    results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "\n",
    "\n",
    "                ##########################################\n",
    "                #######   Salem scenario 1        ########\n",
    "                ##########################################\n",
    "                if \"Salem1\" in scenarios:\n",
    "                    scenario = \"Salem1\"\n",
    "                    mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n",
    "                        target_classifier,\n",
    "                        classifiers[classifier_name](**params),\n",
    "                        X_target_train,\n",
    "                        X_shadow_train,\n",
    "                        y_shadow_train,\n",
    "                        X_test,\n",
    "                        mia_classifier=mia_classifier()\n",
    "                    )\n",
    "\n",
    "                    # Get Shadow and MIA metrics\n",
    "                    shadow_metrics = {f\"shadow_{key}\": val for key, val in get_metrics(shadow_model, X_shadow_test, y_shadow_test).items()}\n",
    "                    mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "\n",
    "                    #Create ID for dataset classifier parameters scenario (but not repetition/random split)\n",
    "                    hashstr = f'{dataset} {classifier_name} {str(params)} {scenario}'\n",
    "                    full_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    new_results = ResultsEntry(\n",
    "                        full_id, model_data_param_id, param_id,\n",
    "                        dataset,\n",
    "                        scenario,\n",
    "                        classifier_name,\n",
    "                        shadow_dataset='Same distribution',\n",
    "                        shadow_classifier_name=classifier_name,\n",
    "                        attack_classifier_name=mia_classifier_name,\n",
    "                        repetition=r,\n",
    "                        params=params,\n",
    "                        target_metrics=target_metrics,\n",
    "                        mia_metrics=mia_metrics,\n",
    "                        shadow_metrics=shadow_metrics\n",
    "                    )\n",
    "\n",
    "                    results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "\n",
    "                ##########################################\n",
    "                #######   Salem scenario 2        ########\n",
    "                ##########################################\n",
    "                if \"Salem2\" in scenarios:\n",
    "                    shadow_dataset = 'Breast cancer'\n",
    "                    scenario = \"Salem2\"\n",
    "\n",
    "                    X_breast_cancer, y_breast_cancer = skl_datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "                    mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n",
    "                        target_classifier,\n",
    "                        classifiers[classifier_name](**params),\n",
    "                        X_target_train,\n",
    "                        X_breast_cancer,\n",
    "                        y_breast_cancer,\n",
    "                        X_test,\n",
    "                        mia_classifier=mia_classifier()\n",
    "                    )\n",
    "\n",
    "                    # Get Shadow and MIA metrics\n",
    "                    shadow_metrics = {f\"shadow_{key}\": val for key, val in get_metrics(shadow_model, X_shadow_test, y_shadow_test).items()}\n",
    "                    mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "\n",
    "                    #Create ID for dataset classifier parameters scenario (but not repetition/random split)\n",
    "                    hashstr = f'{dataset} {classifier_name} {str(params)} {scenario}'\n",
    "                    full_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    new_results = ResultsEntry(\n",
    "                        full_id, model_data_param_id, param_id,\n",
    "                        dataset,\n",
    "                        scenario,\n",
    "                        classifier_name,\n",
    "                        shadow_classifier_name=classifier_name,\n",
    "                        shadow_dataset=shadow_dataset,\n",
    "                        attack_classifier_name=mia_classifier_name,\n",
    "                        repetition=r,\n",
    "                        params=params,\n",
    "                        target_metrics=target_metrics,\n",
    "                        shadow_metrics=shadow_metrics,\n",
    "                        mia_metrics=mia_metrics\n",
    "                    )\n",
    "\n",
    "                    results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "warnings.simplefilter(\"default\")#enable warnings again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df14e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results file\n",
    "results_df.to_csv(results_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
