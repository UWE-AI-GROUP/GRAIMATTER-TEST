{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "# Classifiers for attack models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import t=privachy version \n",
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the datasets\n",
    "1. We draw data points from a distribution.\n",
    "2. We split these data points into the target dataset and a shadow dataset drawn from the same distribution.\n",
    "3. We also draw a dataset from a different distribution.\n",
    "\n",
    "**NOTE**. ***I make datasets with few samples but with many features to force the target model to overfit.***\n",
    "\n",
    "\n",
    "***NOTE JIM: had to make batch_size 25 so DP optimizer would run with same hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "\n",
    "# (X,y): Original distribution\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_classes=n_classes, \n",
    "                           n_features=300,\n",
    "                           n_informative=300,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           random_state=15\n",
    "                          )\n",
    "# One-hot encoding of the label\n",
    "y = np.eye(n_classes)[y]\n",
    "\n",
    "# (Xt, yt) is the target dataset, owned by the TRE and drawn from the (X,y) distribution\n",
    "# (Xs, ys) is a shadow dataset drawn from the (X,y) distribution\n",
    "Xt, Xs, yt, ys = train_test_split(X, y, test_size=0.50, random_state=15)\n",
    "\n",
    "# (Xd, yd) is a shadow dataset, drawn from a different distribution (different seed)\n",
    "Xd, yd = make_classification(n_samples=1000,\n",
    "                           n_classes=n_classes, \n",
    "                           n_features=300,\n",
    "                           n_informative=300,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           random_state=42\n",
    "                          )\n",
    "yd = np.eye(n_classes)[yd]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (member) and test (non-member) datasets\n",
    "# Set shuffle to False so that Xt_membership is consistent with Xt, otherwise\n",
    "# we need to stack Xt_member and Xt_nonmember again to get a consistent Xt.\n",
    "Xt_member, Xt_nonmember, yt_member, yt_nonmember = train_test_split(Xt, yt, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Set membership status for future tests\n",
    "Xt_membership = np.vstack(\n",
    "    (\n",
    "        np.ones((Xt_member.shape[0], 1), np.uint8),\n",
    "        np.zeros((Xt_nonmember.shape[0], 1), np.uint8)\n",
    "    )\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the target model architecture\n",
    "\n",
    "*Again, I'm using a rather big model (for the classification task) to favour overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target model\n",
    "# Tensorflow model (MLP) (making it big to make it overfit)\n",
    "\n",
    "input_data = Input(shape = Xt_member[0].shape)\n",
    "x = Dense(128, activation='relu')(input_data)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try the SafeKerasModel version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import safemodel\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=None\n",
    "safeModel = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekeras-test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safeModel.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "safeModel.compile(loss=loss)#optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theType= type(safeModel.optimizer)\n",
    "print(f'optimiser is type {theType}')\n",
    "\n",
    "dpused,reason = safeModel.check_optimizer_is_DP(safeModel.optimizer)\n",
    "print(f' It is {dpused} that the moddel will be DP because {reason}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 25\n",
    "\n",
    "r_DP = safeModel.fit(Xt_member, \n",
    "              yt_member, \n",
    "              validation_data=(Xt_nonmember, yt_nonmember),\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size\n",
    ")  \n",
    "\n",
    "\n",
    "plt.plot(r_DP.history['accuracy'], label='accuracy')\n",
    "plt.plot(r_DP.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theType= type(safeModel.optimizer)\n",
    "print(f'optimiser is type {theType}')\n",
    "\n",
    "dpused,reason = safeModel.check_DP_used(safeModel.optimizer)\n",
    "print(f' It is {dpused} that the model will be DP because {reason}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dataset has {Xt.shape[0]} entries so batch size is {100*safeModel.batch_size/Xt.shape[0]}%')\n",
    "privacy = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=Xt.shape[0],\n",
    "                                              batch_size=safeModel.batch_size,\n",
    "                                              noise_multiplier=safeModel.noise_multiplier,\n",
    "                                              epochs=25,#epochs,\n",
    "                                              delta=1e-5)\n",
    "print(f'with these settings privacy = {privacy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.save('safekeras.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.preliminary_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in safeModel.__dict__.items():\n",
    "     print (f'thing associated with key {key} has type {type(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = []\n",
    "unusuals =[]\n",
    "tuples =[]\n",
    "deleted_keys = []\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Tuples\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == tuple):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "        tuples.append(key)\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Bools\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == bool):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Lists\")\n",
    "print(\"===============================================================================================\")        \n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == list):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Strings\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == str):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "        \n",
    "print(\"===============================================================================================\")\n",
    "print(\"Ints\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == int):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Floats\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == float):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "        \n",
    "print(\"===============================================================================================\")\n",
    "print(\"Dicts\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == dict):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "        \n",
    "print(\"===============================================================================================\")\n",
    "print(\"Sets\")\n",
    "print(\"===============================================================================================\")\n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if(type(value) == set):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "\n",
    "print(\"===============================================================================================\")\n",
    "print(\"Unusual Ones\")\n",
    "print(\"===============================================================================================\")\n",
    "\n",
    "        \n",
    "for key,value in safeModel.__dict__.items():\n",
    "    if((type(value) != int) and (type(value) != str) and \n",
    "       (type(value) != list) and (type(value) != bool) and \n",
    "       (type(value) != tuple) and (type(value) != float) and\n",
    "       (type(value) != dict) and (type(value) != set)):\n",
    "        print (f'thing associated with key {key} has type {type(value)}')\n",
    "        unusuals.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safeModel.request_release('safekeras.pkl') # TypeError: cannot pickle '_thread.RLock' object\n",
    "\n",
    "#safeModel.request_release('safekeras.sav') # TypeError: cannot pickle '_thread.RLock' object\n",
    "\n",
    "#safeModel.request_release('safekeras.tf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "safeModel.save('my_model')\n",
    "safeModel.save_weights('weights.h5')\n",
    "\n",
    "safeModel.request_release('safe.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "safeModel.request_release('safekeras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and fit using recommended params\n",
    "print(\"***Test 1: researcher doesn't change recommended params\")\n",
    "optimizer=None\n",
    "safeKerasModel1 = Safe_KerasModel(inputs=input_data, outputs=output,name=\"safekerasmodel1-test\")\n",
    "safeKerasModel1.compile(loss=loss)\n",
    "safeKerasModel1.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel1.save(name=\"safe1.pkl\")\n",
    "safeKerasModel1.preliminary_check()\n",
    "safeKerasModel1.request_release(filename=\"safe1.pkl\")\n",
    "\n",
    "\n",
    "# change model params to recommended values\n",
    "print(\"\\n***Test 2: researcher changes params safely\")\n",
    "safeKerasModel2 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel2-test\")\n",
    "safeKerasModel2.compile(loss=loss)\n",
    "safeKerasModel2.optimizer=\"DPAdam\"\n",
    "safeKerasModel2.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel2.save(name=\"safe2.pkl\")\n",
    "safeKerasModel2.preliminary_check()\n",
    "safeKerasModel2.request_release(filename=\"safe2.pkl\")\n",
    "\n",
    "# change one model params in an unsafe way\n",
    "print(\"\\n***Test 3: researcher changes string params unsafely\")\n",
    "safeKerasModel3 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel3-test\")\n",
    "safeKerasModel3.compile()\n",
    "safeKerasModel3.optimizer=\"Adam\"\n",
    "safeKerasModel3.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel3.save(name=\"unsafe3.pkl\")\n",
    "safeKerasModel3.preliminary_check()\n",
    "safeKerasModel3.request_release(filename=\"unsafe3.pkl\")\n",
    "\n",
    "# change another model params in an  unsafe way\n",
    "\n",
    "\n",
    "# change another model params in an  unsafe way\n",
    "print(\"\\n***Test 5: researcher changes string and numeric params unsafely\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change another model params in an  unsafe way but tells preliminary_check() not to overwrite params\n",
    "print(\"\\n***Test 6: researcher changes string and numeric params unsafely\")\n",
    "safeKerasModel6 = Safe_KerasModel(inputs= input_data, outputs=output,name=\"safekerasmodel6-test\")\n",
    "safeKerasModel6.compile()\n",
    "safeKerasModel6.optimizer=\"Adam\"\n",
    "safeKerasModel6.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel6.save(name=\"unsafe6.pkl\")\n",
    "safeKerasModel6.preliminary_check(apply_constraints=False)\n",
    "safeKerasModel6.request_release(filename=\"unsafe6.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
