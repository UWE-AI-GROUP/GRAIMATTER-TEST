{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cdcf9e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scenarios import *\n",
    "from plots import *\n",
    "from metrics import get_metrics\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "#print(PROJECT_ROOT)\n",
    "from data_preprocessing.data_interface import get_data_sklearn, DataNotAvailable\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets as skl_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "absolute-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_info:\n",
    "    '''\n",
    "    For each model it builds a class where information can be more easily identified.\n",
    "    \n",
    "    kind: type of model, can be either target, shadow or attack\n",
    "    name: name of the classifier\n",
    "    dataset: name of the dataset used to train the model\n",
    "    params: parameters used for the model\n",
    "    attack_scenario: if this is part of an attack scenario, state which scenario name\n",
    "    metrics: calculated metrics for the model\n",
    "    \n",
    "    To view the data contained in the instance use model_info.show().\n",
    "    '''\n",
    "    def __init__(self, target_model_id, kind, classifier, dataset, params=None, attack_scenario=None, metrics=None):\n",
    "        self.target_model_id = target_model_id #identifier of the target model, so it makes possible to map attacks and target models\n",
    "        self.kind = kind #type of model: target, shadow, attack\n",
    "        self.classifier = classifier #name of the classifier\n",
    "        self.dataset = dataset #name of the dataset used to train the model\n",
    "        self.params = params #parameters used. 'None' assumes default params\n",
    "        self.attack_scenario = attack_scenario #Only relevant to the attack\n",
    "        self.metrics = metrics #calculated metrics, expect a dictionary\n",
    "    \n",
    "    def show(self):\n",
    "        print(\"Target model ID:\", self.target_model_id)\n",
    "        print(\"Type:\", self.kind)\n",
    "        print(\"Classifier:\", self.classifier)\n",
    "        print(\"Parameters:\", self.params)\n",
    "        print(\"Dataset:\", self.dataset)\n",
    "        print(\"Attack scenario:\", self.attack_scenario)\n",
    "        print(\"Metrics\")\n",
    "        for metric, value in self.metrics.items():\n",
    "            print(metric, value)\n",
    "    \n",
    "    def data_frame(self):\n",
    "        d = {\"Target model ID\":self.target_model_id, \"Type\": self.kind, \"Classifier\":self.classifier,\n",
    "                          \"Dataset\":self.dataset, \"Attack scenario\": self.attack_scenario}\n",
    "        #print(self.params, type(self.params))\n",
    "        if not self.params:\n",
    "            self.params = {}\n",
    "        return(pd.DataFrame.from_dict({**d, **self.params, **self.metrics}, orient='index').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "designed-andrew",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "swedish-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path:str):\n",
    "    \"\"\"\n",
    "    Creates a new directory if it does not exist.\n",
    "\n",
    "    path: directory to create.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-metabolism",
   "metadata": {},
   "source": [
    "Create a directory to save images and results files if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "structural-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join(PROJECT_ROOT, 'results')\n",
    "create_dir(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-reception",
   "metadata": {},
   "source": [
    "Define available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fundamental-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'mimic2-iaccd',\n",
    "    'in-hospital-mortality',\n",
    "    'medical-mnist-ab-v-br-100',\n",
    "    'indian liver',\n",
    "    'texas hospitals 10'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "79219b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "        'RandomForestClassifier':RandomForestClassifier, #bootstrap=False\n",
    "        'DecisionTreeClassifier':DecisionTreeClassifier,\n",
    "        'GaussianProcessClassifier':GaussianProcessClassifier,\n",
    "        'MLPClassifier':MLPClassifier,\n",
    "        'KNeighborsClassifier':KNeighborsClassifier,\n",
    "        'SVC':SVC,#kernel='rbf', probability=True),\n",
    "        'AdaBoostClassifier':AdaBoostClassifier #n_estimators=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "63b3cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'RandomForestClassifier': {\n",
    "        #'n_estimators': [10, 20, 100],\n",
    "        #'criterion':['gini','entropy'],\n",
    "        #'max_depth':[None,2,4],\n",
    "        #'max_features':[None,'sqrt','log2'],\n",
    "        'bootstrap': [True, False],\n",
    "        'min_samples_split': [2, 10],\n",
    "        #'class_weight':[None,'balanced','balanced_subsample'],\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        #'criterion':['gini','entropy'],\n",
    "        'max_depth':[None,2,4],\n",
    "        #'min_samples_split': [2, 10],\n",
    "        #'max_features':[None,'sqrt','log2'],\n",
    "        #'class_weight':[None,'balanced']\n",
    "    },\n",
    "    'GaussianProcessClassifier': {\n",
    "        'max_iter_predict':[50,100,200],\n",
    "        'warm_start':[True,False],\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        #'hidden_layer_size':[(50,),(100,),(200,)],\n",
    "        #'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver':['lbfgs', 'sgd', 'adam'],\n",
    "        #'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        #'max_iter': [50,200,400,1000]\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors':[2,5,10,20],\n",
    "        'weights':['uniform', 'distance'],\n",
    "        #'algorithm':['ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "    'SVC': {\n",
    "        #'Kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        #'decision_function_shape':['ovo', 'ovr'],\n",
    "        #'max_iter':[-1, 2, 5],\n",
    "        'probability':[True]\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        #'n_estimators': [10, 20, 50, 100],\n",
    "        'algorithm':['SAMME', 'SAMME.R']\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "033921b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsEntry():\n",
    "    def __init__(self, dataset_name, scenario_name, classifier_name, shadow_classifier_name=None, shadow_dataset=None, params={}, target_metrics={}, shadow_metrics={}, mia_metrics={}):\n",
    "        self.metadata = {\n",
    "            'dataset': dataset_name,\n",
    "            'scenario': scenario_name,\n",
    "            'target_classifier': classifier_name,\n",
    "            'shadow_classifier_name': shadow_classifier_name,\n",
    "            'shadow_dataset': shadow_dataset\n",
    "        }\n",
    "        self.params = params\n",
    "        self.target_metrics = target_metrics\n",
    "        self.shadow_metrics = shadow_metrics\n",
    "        self.mia_metrics = mia_metrics\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        return(\n",
    "            pd.DataFrame.from_dict(\n",
    "                {\n",
    "                    **self.metadata,\n",
    "                    **self.params,\n",
    "                    **self.target_metrics,\n",
    "                    **self.mia_metrics,\n",
    "                    **self.shadow_metrics\n",
    "                }, orient='index').T\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "relevant-tours",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:C:\\Users\\simonr04\\git\\GRAIMatter\\data_preprocessing\\data_interface.py:DATASET FOLDER = C:\\Users\\simonr04\\git\\GRAIMatter\\data\n",
      "INFO:C:\\Users\\simonr04\\git\\GRAIMatter\\data_preprocessing\\data_interface.py:Loading mimic2-iaccd\n",
      "INFO:C:\\Users\\simonr04\\git\\GRAIMatter\\data_preprocessing\\data_interface.py:Preprocessing\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['NPV'] = tn / (tn + fn) #negative predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:46: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['NLR'] = metrics['FNR'] / metrics['TNR'] #negative likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  metrics['OR'] = metrics['PLR'] / metrics['NLR'] #odds ratio, the odds ratio is used to find the probability of an outcome of an event when there are two possible outcomes\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['FAR'] = fp / (fp + tp) #proportion of things classified as positives that are incorrect, also known as false discovery rate\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:40: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  metrics['PPV'] = tp / (tp + fp) #precision or positive predictive value\n",
      "C:\\Users\\simonr04\\git\\GRAIMatter\\WP1\\notebooks\\metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  metrics['PLR'] = metrics['TPR'] / metrics['FPR'] #positive likelihood ratio\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MIA_CLASSIFIER_NAME = \"RandomForestClassifier\"\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets[:1]:\n",
    "    #load the data\n",
    "    try:\n",
    "        X, y = get_data_sklearn(dataset)\n",
    "    except DataNotAvailable as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    #split into training, shadow model and validation data\n",
    "    X_target_train, X_shadow_train, X_test, y_target_train, y_shadow_train, y_test = split_target_data(X.values, y.values)\n",
    "        \n",
    "    for classifier_name, clf_class in classifiers.items():\n",
    "        all_combinations = itertools.product(*experiment_params[classifier_name].values())\n",
    "        for i,combination in enumerate(all_combinations):\n",
    "            \n",
    "            # Turn this particular combination into a dictionary\n",
    "            params = {n: v for n, v in zip(experiment_params[classifier_name].keys(), combination)}\n",
    "            target_classifier = clf_class()\n",
    "            target_classifier.set_params(**params)\n",
    "            \n",
    "            # Train the target model\n",
    "            target_classifier.fit(X_target_train, y_target_train)\n",
    "            \n",
    "            # Get target metrics\n",
    "            target_metrics = {f\"target_{key}\": val for key, val in get_metrics(target_classifier, X_test, y_test).items()}\n",
    "            \n",
    "            ##########################################\n",
    "            #######   Worst case scenario     ########\n",
    "            ##########################################\n",
    "            \n",
    "            scenario = \"Worst Case\"\n",
    "            mi_test_x, mi_test_y, mi_clf = worst_case_mia(\n",
    "                target_classifier,\n",
    "                X_target_train,\n",
    "                X_test,\n",
    "                mia_classifier=RandomForestClassifier()\n",
    "            )\n",
    "            # Get MIA metrics\n",
    "            mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "            \n",
    "            new_results = ResultsEntry(\n",
    "                dataset,\n",
    "                scenario,\n",
    "                classifier_name,\n",
    "                params=params,\n",
    "                target_metrics=target_metrics,\n",
    "                mia_metrics=mia_metrics\n",
    "            )\n",
    "            \n",
    "            results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "            \n",
    "            \n",
    "            ##########################################\n",
    "            #######   Salem scenario 1        ########\n",
    "            ##########################################\n",
    "            \n",
    "            scenario = \"Salem1\"\n",
    "            mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n",
    "                target_classifier,\n",
    "                classifiers[classifier_name](**params),\n",
    "                X_target_train,\n",
    "                X_shadow_train,\n",
    "                y_shadow_train,\n",
    "                X_test,\n",
    "                mia_classifier=RandomForestClassifier()\n",
    "            )\n",
    "            \n",
    "            # Get Shadow and MIA metrics\n",
    "            shadow_metrics = {f\"shadow_{key}\": val for key, val in get_metrics(shadow_model, X_shadow_test, y_shadow_test).items()}\n",
    "            mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "            \n",
    "            new_results = ResultsEntry(\n",
    "                dataset,\n",
    "                scenario,\n",
    "                classifier_name,\n",
    "                shadow_dataset='Same distribution',\n",
    "                shadow_classifier_name = classifier_name\n",
    "                params=params,\n",
    "                target_metrics=target_metrics,\n",
    "                mia_metrics=mia_metrics,\n",
    "                shadow_metrics=shadow_metrics,\n",
    "            )\n",
    "\n",
    "            results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "\n",
    "            ##########################################\n",
    "            #######   Salem scenario 2        ########\n",
    "            ##########################################\n",
    "            \n",
    "            shadow_dataset = 'Breast cancer'\n",
    "            scenario = \"Salem2\"\n",
    "            \n",
    "            X_breast_cancer, y_breast_cancer = skl_datasets.load_breast_cancer(return_X_y=True)\n",
    "            \n",
    "            mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n",
    "                target_classifier,\n",
    "                classifiers[classifier_name](**params),\n",
    "                X_target_train,\n",
    "                X_breast_cancer,\n",
    "                y_breast_cancer,\n",
    "                X_test,\n",
    "                mia_classifier=RandomForestClassifier()\n",
    "            )\n",
    "            \n",
    "            # Get Shadow and MIA metrics\n",
    "            shadow_metrics = {f\"shadow_{key}\": val for key, val in get_metrics(shadow_model, X_shadow_test, y_shadow_test).items()}\n",
    "            mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "            \n",
    "            new_results = ResultsEntry(\n",
    "                dataset,\n",
    "                scenario,\n",
    "                classifier_name,\n",
    "                shadow_classifier_name = classifier_name,\n",
    "                shadow_dataset=shadow_dataset,\n",
    "                params=params,\n",
    "                target_metrics=target_metrics,\n",
    "                shadow_metrics=shadow_metrics,\n",
    "                mia_metrics=mia_metrics\n",
    "            )\n",
    "            \n",
    "            results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4174c228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>scenario</th>\n",
       "      <th>target_classifier</th>\n",
       "      <th>shadow_dataset</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>target_TPR</th>\n",
       "      <th>target_FPR</th>\n",
       "      <th>target_FAR</th>\n",
       "      <th>target_TNR</th>\n",
       "      <th>...</th>\n",
       "      <th>shadow_NLR</th>\n",
       "      <th>shadow_OR</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_iter_predict</th>\n",
       "      <th>warm_start</th>\n",
       "      <th>solver</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>probability</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mimic2-iaccd</td>\n",
       "      <td>Worst Case</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mimic2-iaccd</td>\n",
       "      <td>Salem1</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Same distribution</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mimic2-iaccd</td>\n",
       "      <td>Worst Case</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mimic2-iaccd</td>\n",
       "      <td>Salem1</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Same distribution</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mimic2-iaccd</td>\n",
       "      <td>Salem2</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Breast cancer</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset    scenario       target_classifier     shadow_dataset  \\\n",
       "0  mimic2-iaccd  Worst Case  RandomForestClassifier               None   \n",
       "1  mimic2-iaccd      Salem1  RandomForestClassifier  Same distribution   \n",
       "6  mimic2-iaccd  Worst Case  RandomForestClassifier               None   \n",
       "7  mimic2-iaccd      Salem1  RandomForestClassifier  Same distribution   \n",
       "8  mimic2-iaccd      Salem2  RandomForestClassifier      Breast cancer   \n",
       "\n",
       "  bootstrap min_samples_split target_TPR target_FPR target_FAR target_TNR  \\\n",
       "0      True                 2        1.0   0.014925   0.005882   0.985075   \n",
       "1      True                 2        1.0   0.014925   0.005882   0.985075   \n",
       "6     False                 2        1.0        0.0        0.0        1.0   \n",
       "7     False                 2        1.0        0.0        0.0        1.0   \n",
       "8     False                 2        1.0        0.0        0.0        1.0   \n",
       "\n",
       "   ... shadow_NLR shadow_OR max_depth max_iter_predict warm_start solver  \\\n",
       "0  ...        NaN       NaN       NaN              NaN        NaN    NaN   \n",
       "1  ...        0.0       inf       NaN              NaN        NaN    NaN   \n",
       "6  ...        NaN       NaN       NaN              NaN        NaN    NaN   \n",
       "7  ...        0.0       inf       NaN              NaN        NaN    NaN   \n",
       "8  ...   0.012144    1089.0       NaN              NaN        NaN    NaN   \n",
       "\n",
       "  n_neighbors weights probability algorithm  \n",
       "0         NaN     NaN         NaN       NaN  \n",
       "1         NaN     NaN         NaN       NaN  \n",
       "6         NaN     NaN         NaN       NaN  \n",
       "7         NaN     NaN         NaN       NaN  \n",
       "8         NaN     NaN         NaN       NaN  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df.mia_TPR > 0.7].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "swiss-country",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', 'scenario', 'target_classifier', 'shadow_dataset',\n",
       "       'bootstrap', 'min_samples_split', 'target_TPR', 'target_FPR',\n",
       "       'target_FAR', 'target_TNR', 'target_PPV', 'target_NPV', 'target_FNR',\n",
       "       'target_ACC', 'target_Advantage', 'target_PLR', 'target_NLR',\n",
       "       'target_OR', 'mia_TPR', 'mia_FPR', 'mia_FAR', 'mia_TNR', 'mia_PPV',\n",
       "       'mia_NPV', 'mia_FNR', 'mia_ACC', 'mia_Advantage', 'mia_PLR', 'mia_NLR',\n",
       "       'mia_OR', 'shadow_TPR', 'shadow_FPR', 'shadow_FAR', 'shadow_TNR',\n",
       "       'shadow_PPV', 'shadow_NPV', 'shadow_FNR', 'shadow_ACC',\n",
       "       'shadow_Advantage', 'shadow_PLR', 'shadow_NLR', 'shadow_OR',\n",
       "       'max_depth', 'max_iter_predict', 'warm_start', 'solver', 'n_neighbors',\n",
       "       'weights'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for s, v in sets.items():\n",
    "    df = pd.concat([v.data_frame(),df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-stopping",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby(['Target model ID', 'Classifier', 'Attack scenario', 'Type', \"Dataset\"])['TPR', 'FPR',\n",
    "                                                                                   'FAR', 'TNR', \n",
    "                                                                                    'PPV', 'NPV',\n",
    "                                                                                   'FNR', 'ACC',\n",
    "                                                                                   'Advantage',\n",
    "                                                                                   ].sum()#.reset_index()\n",
    "                                                                                    #'PLR', 'NLR',\n",
    "                                                                                    #'OR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(min_samples_split = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b912b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad4cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
