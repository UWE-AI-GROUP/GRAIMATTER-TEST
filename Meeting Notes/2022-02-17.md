# WP1+WP2 catch-up Agenda 22nd Feb

Attendance (Alphabetic List)

* Alberto Blanco  - present
* Christian Cole  - preent
* Alba Crespi  - present
* Emily Jefferson  - present
* James Liley  - present
* Maeve Malone  - present
* Esma Mansouri-Benssassi - present
* Andrew McCarthy  - present
* Richard Preen  - preent
* Felix Ritchie  - no
* Simon Rogers - present t
* Jim Smith - (Chair/Minutes)
* Francesco Tava - present
* Josep Domingo-Ferrer- present

1. Apologies etc 

2. WP1 update

  2.1 Summary
  Alba automating things
  
  2.2 WP1 Open issues https://github.com/jim-smith/GRAIMatter/issues.  
     Jim's grouping of many issues is:
     Designing and implementing code framework for automatically running and collating results from experiments across combinations of:
      - [datasets| parameters for tunable datasets]  where MIA can be shown to be an issue
        -- texas dataset, merging things sorting thme out,
        -- looking at different metrics 
      - machine learning algorithms
      - hyper-parameter grids
      - standard vs. DP variants vs. "Safe_X" when written
     - attack types (Salem variants)
     - attack tools (worst-case, privacy meter, ART, tenasorflow, etc.)
    Status: maybe a week away,   AWS access is ready so everything is coming together nicely to run some big sets of experiments
 
  Metrics --next week arrange a dedicated session to discuss what other types of attack we should look at.
 
  - synthetic data?
    -- have reached a consensus on how to proceed. James has found new new github package (see meeting chat).  Chris has some tools that HIC use to create synthetic datasets. One in house (BADMEdicine) , one from OHDSSI called "whiterabbit?" makes synthetic datasets based on univariate marginal distributions.
    
* Esma and Chris have set up the AWS and we just need to let them know we want access.

3. WP2 Update

   3.1 Summary. 

   3.2 WP2 Open Issues. https://github.com/jim-smith/GRAIMatter/issues
     - Tweaks to Safemodel class
     - Implementing different classes
     - Dealing with combined conditions
     - Static code analyser
     - Hooks to run attack code automatically??
     - Boundary between careless and mailicous users 
     ** kicked upstairs
     * Richard has been looking at ML_Doctor in particular - depends a lot on pytorch  
   
   3.3 Obstacles/Problems


4. Should TREs keep some data from researchers (Simon to lead)
* so TREs can run risk assessments
* add to agenda for mid-March meeting, 
* would it be palatable for TREs and researchers?
* benefit - keep a some validation data and how TREs monitor different algorithms
*Action: devise software tools to do that setting aside of validation data.
* Question How do we flag to people that they should not be using high capacity models for small data


5. Documentation (including glossary) : ALL
* just keep ongoing

6. Timelines for next two weeks


* Closing off issues on github?


7. Revising  / resubmitting the paper

* lots of useful comments from reviewers
* Emily discussed the style/tone of the paper - not really just a review, it is viewing the field through a different lens.    
* Simon has added reference in chat of all the relevant risks from a very ML perspective.



8. AOB
