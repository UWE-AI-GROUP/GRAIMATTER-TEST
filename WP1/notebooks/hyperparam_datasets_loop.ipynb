{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a469e5",
   "metadata": {},
   "source": [
    "# Notebook to loop over datasets, models, scenarios and hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcf9e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data_preprocessing/data_interface.py:ROOT PROJECT FOLDER = /home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "import hashlib\n",
    "import itertools\n",
    "import warnings\n",
    "import logging\n",
    "import pandas as pd\n",
    "from metrics import get_metrics\n",
    "from sklearn import datasets as skl_datasets\n",
    "from scenarios import *\n",
    "from plots import *\n",
    "from experiments import ResultsEntry\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "#print(PROJECT_ROOT)\n",
    "from data_preprocessing.data_interface import get_data_sklearn, DataNotAvailable\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa6ed0",
   "metadata": {},
   "source": [
    "Load the experimental config from the config file. The config file should include a single dictionary with the following keys:\n",
    "1. `datasets`: a list of the dataset names (strings) to use\n",
    "1. `classifiers`: a list of the classifiers to use. Each item is itself a list with two elements, the first being the string of the module (e.g. `\"sklearn.svm\"`) and the second being the class name (e.g. `\"SVC\"`)\n",
    "1. `experiment_params`: A dictionary where each classifier class name from above is a key and the value is another dictionary where the keys are the hyperparameters to vary and the values are lists over the values the hyper-parameter should take. Note in json, use `true, false, null` instead of `True, False, None`.\n",
    "1. `results_filename`: string of a filename in which to save the results. Will overwrite happily.\n",
    "1. `n_reps`: how many times to repeat each combination of dataset, model, scenario, hyper-parameter\n",
    "1. `mia_classifier`: the classifier to use for the attack model. Currently restricted to default hyper-params. Should be specified as a list with two items, as per `classifiers`\n",
    "1. `scenarios`: a list of the scenarios to run. Can only include a subset of \"WorstCase\", \"Salem1\", and \"Salem2\" at the moment\n",
    "\n",
    "An example config file is provided as `example_loop_experiment_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ed8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG_FILENAME = \"example_loop_experiment_config.json\"\n",
    "CONFIG_FILENAME = \"randomForest_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FILENAME, 'r') as f:\n",
    "    config = json.loads(f.read())\n",
    "\n",
    "datasets = config['datasets']\n",
    "classifier_strings = config['classifiers']\n",
    "\n",
    "classifiers = {}\n",
    "for module_name, class_name in classifier_strings:\n",
    "    module = importlib.import_module(module_name)\n",
    "    class_ = getattr(module, class_name)\n",
    "    classifiers[class_name] = class_\n",
    "\n",
    "experiment_params = config['experiment_params']\n",
    "\n",
    "results_filename = config['results_filename']\n",
    "\n",
    "n_reps = config['n_reps']\n",
    "\n",
    "mia_classifier_module, mia_classifier_name = config['mia_classifier']\n",
    "module = importlib.import_module(mia_classifier_module)\n",
    "mia_classifier = getattr(module, mia_classifier_name)\n",
    "\n",
    "scenarios = config['scenarios']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81ad45",
   "metadata": {},
   "source": [
    "@Alba: didn't remove this in case you wanted to keep all those hyper-param lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b3cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_params = {\n",
    "#     'RandomForestClassifier': {\n",
    "#         #'n_estimators': [10, 20, 100],\n",
    "#         #'criterion':['gini','entropy'],\n",
    "#         #'max_depth':[None,2,4],\n",
    "#         #'max_features':[None,'sqrt','log2'],\n",
    "#         'bootstrap': [True, False],\n",
    "#         'min_samples_split': [2, 10],\n",
    "#         #'class_weight':[None,'balanced','balanced_subsample'],\n",
    "#     },\n",
    "#     'DecisionTreeClassifier': {\n",
    "#         #'criterion':['gini','entropy'],\n",
    "#         'max_depth':[None,2,4],\n",
    "#         #'min_samples_split': [2, 10],\n",
    "#         #'max_features':[None,'sqrt','log2'],\n",
    "#         #'class_weight':[None,'balanced']\n",
    "#     },\n",
    "#     'GaussianProcessClassifier': {\n",
    "#         'max_iter_predict':[50,100,200],\n",
    "#         'warm_start':[True,False],\n",
    "#     },\n",
    "#     'MLPClassifier': {\n",
    "#         #'hidden_layer_size':[(50,),(100,),(200,)],\n",
    "#         #'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "#         'solver':['lbfgs', 'sgd', 'adam'],\n",
    "#         #'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "#         #'max_iter': [50,200,400,1000]\n",
    "#     },\n",
    "#     'KNeighborsClassifier': {\n",
    "#         'n_neighbors':[2,5,10,20],\n",
    "#         'weights':['uniform', 'distance'],\n",
    "#         #'algorithm':['ball_tree', 'kd_tree', 'brute']\n",
    "#     },\n",
    "#     'SVC': {\n",
    "#         #'Kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#         #'decision_function_shape':['ovo', 'ovr'],\n",
    "#         #'max_iter':[-1, 2, 5],\n",
    "#         'probability':[True]\n",
    "#     },\n",
    "#     'AdaBoostClassifier': {\n",
    "#         #'n_estimators': [10, 20, 50, 100],\n",
    "#         'algorithm':['SAMME', 'SAMME.R']\n",
    "#     }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "relevant-tours",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data_preprocessing/data_interface.py:DATASET FOLDER = /home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data\n",
      "INFO:/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data_preprocessing/data_interface.py:Loading mimic2-iaccd\n",
      "INFO:/home/alba/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/data_preprocessing/data_interface.py:Preprocessing\n",
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic2-iaccd\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ff45f92129b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"Salem1\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscenarios\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Salem1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n\u001b[0m\u001b[1;32m     81\u001b[0m                         \u001b[0mtarget_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/WP1/notebooks/scenarios.py\u001b[0m in \u001b[0;36msalem\u001b[0;34m(target_model, shadow_clf, X_target_train, X_shadow, y_shadow, X_test, prop_shadow_train, mia_classifier)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mmia_train_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mia_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshadow_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_shadow_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_shadow_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mmia_test_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmia_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mia_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_target_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HIC_Dundee/GRAIMAtter/GRAIMatter/WP1/notebooks/scenarios.py\u001b[0m in \u001b[0;36mcreate_mia_data\u001b[0;34m(clf, xtrain, xtest, sort_probs, keep_top)\u001b[0m\n\u001b[1;32m     52\u001b[0m         (\n\u001b[1;32m     53\u001b[0m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    859\u001b[0m         ]\n\u001b[1;32m    860\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         Parallel(\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    354\u001b[0m         self.parallel._backend.batch_completed(self.batch_size,\n\u001b[1;32m    355\u001b[0m                                                this_batch_duration)\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"once\")\n",
    "    #MPLClassifier is giving a lot of warnings. \n",
    "    # For each repetition are the same, so it will only show the same warning once.\n",
    "\n",
    "for dataset in datasets:#[:1]:\n",
    "    #load the data\n",
    "    try:\n",
    "        X, y = get_data_sklearn(dataset)\n",
    "    except DataNotAvailable as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    print(dataset)\n",
    "    for r in range(n_reps):\n",
    "        #split into training, shadow model and validation data\n",
    "        X_target_train, X_shadow_train, X_test, y_target_train, y_shadow_train, y_test = split_target_data(X.values, y.values, r_state=r)\n",
    "        \n",
    "        for classifier_name, clf_class in classifiers.items():\n",
    "            all_combinations = itertools.product(*experiment_params[classifier_name].values())\n",
    "            for i,combination in enumerate(all_combinations):\n",
    "\n",
    "                # Turn this particular combination into a dictionary\n",
    "                params = {n: v for n, v in zip(experiment_params[classifier_name].keys(), combination)}\n",
    "                target_classifier = clf_class()\n",
    "                target_classifier.set_params(**params)\n",
    "\n",
    "                # Train the target model\n",
    "                target_classifier.fit(X_target_train, y_target_train)\n",
    "\n",
    "                # Get target metrics\n",
    "                target_metrics = {f\"target_{key}\": val for key, val in get_metrics(target_classifier, X_test, y_test).items()}\n",
    "                \n",
    "               \n",
    "                hashstr = f'{dataset} {classifier_name} {str(params)}'\n",
    "                model_data_param_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "                \n",
    "                hashstr = f'{str(params)}'\n",
    "                param_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "                \n",
    "                ##########################################\n",
    "                #######   Worst case scenario     ########\n",
    "                ##########################################\n",
    "                if \"WorstCase\" in scenarios:\n",
    "                    scenario = \"WorstCase\"\n",
    "                    mi_test_x, mi_test_y, mi_clf = worst_case_mia(\n",
    "                        target_classifier,\n",
    "                        X_target_train,\n",
    "                        X_test,\n",
    "                        mia_classifier=mia_classifier()\n",
    "                    )\n",
    "                    # Get MIA metrics\n",
    "                    mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "\n",
    "                    #Create ID for dataset classifier parameters scenario (but not repetition/random split)\n",
    "                    hashstr = f'{dataset} {classifier_name} {str(params)} {scenario}'\n",
    "                    full_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    new_results = ResultsEntry(\n",
    "                        full_id, model_data_param_id, param_id,\n",
    "                        dataset,\n",
    "                        scenario,\n",
    "                        classifier_name,\n",
    "                        attack_classifier_name=mia_classifier_name,\n",
    "                        repetition=r,\n",
    "                        params=params,\n",
    "                        target_metrics=target_metrics,\n",
    "                        mia_metrics=mia_metrics\n",
    "                    )\n",
    "\n",
    "                    results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "\n",
    "\n",
    "                ##########################################\n",
    "                #######   Salem scenario 1        ########\n",
    "                ##########################################\n",
    "                if \"Salem1\" in scenarios:\n",
    "                    scenario = \"Salem1\"\n",
    "                    mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n",
    "                        target_classifier,\n",
    "                        classifiers[classifier_name](**params),\n",
    "                        X_target_train,\n",
    "                        X_shadow_train,\n",
    "                        y_shadow_train,\n",
    "                        X_test,\n",
    "                        mia_classifier=mia_classifier()\n",
    "                    )\n",
    "\n",
    "                    # Get Shadow and MIA metrics\n",
    "                    shadow_metrics = {f\"shadow_{key}\": val for key, val in get_metrics(shadow_model, X_shadow_test, y_shadow_test).items()}\n",
    "                    mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "\n",
    "                    #Create ID for dataset classifier parameters scenario (but not repetition/random split)\n",
    "                    hashstr = f'{dataset} {classifier_name} {str(params)} {scenario}'\n",
    "                    full_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    new_results = ResultsEntry(\n",
    "                        full_id, model_data_param_id, param_id,\n",
    "                        dataset,\n",
    "                        scenario,\n",
    "                        classifier_name,\n",
    "                        shadow_dataset='Same distribution',\n",
    "                        shadow_classifier_name=classifier_name,\n",
    "                        attack_classifier_name=mia_classifier_name,\n",
    "                        repetition=r,\n",
    "                        params=params,\n",
    "                        target_metrics=target_metrics,\n",
    "                        mia_metrics=mia_metrics,\n",
    "                        shadow_metrics=shadow_metrics\n",
    "                    )\n",
    "\n",
    "                    results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "\n",
    "                ##########################################\n",
    "                #######   Salem scenario 2        ########\n",
    "                ##########################################\n",
    "                if \"Salem2\" in scenarios:\n",
    "                    shadow_dataset = 'Breast cancer'\n",
    "                    scenario = \"Salem2\"\n",
    "\n",
    "                    X_breast_cancer, y_breast_cancer = skl_datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "                    mi_test_x, mi_test_y, mi_clf, shadow_model, X_shadow_test, y_shadow_test = salem(\n",
    "                        target_classifier,\n",
    "                        classifiers[classifier_name](**params),\n",
    "                        X_target_train,\n",
    "                        X_breast_cancer,\n",
    "                        y_breast_cancer,\n",
    "                        X_test,\n",
    "                        mia_classifier=mia_classifier()\n",
    "                    )\n",
    "\n",
    "                    # Get Shadow and MIA metrics\n",
    "                    shadow_metrics = {f\"shadow_{key}\": val for key, val in get_metrics(shadow_model, X_shadow_test, y_shadow_test).items()}\n",
    "                    mia_metrics = {f\"mia_{key}\": val for key, val in get_metrics(mi_clf, mi_test_x, mi_test_y).items()}\n",
    "\n",
    "                    #Create ID for dataset classifier parameters scenario (but not repetition/random split)\n",
    "                    hashstr = f'{dataset} {classifier_name} {str(params)} {scenario}'\n",
    "                    full_id = hashlib.sha256(hashstr.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    new_results = ResultsEntry(\n",
    "                        full_id, model_data_param_id, param_id,\n",
    "                        dataset,\n",
    "                        scenario,\n",
    "                        classifier_name,\n",
    "                        shadow_classifier_name=classifier_name,\n",
    "                        shadow_dataset=shadow_dataset,\n",
    "                        attack_classifier_name=mia_classifier_name,\n",
    "                        repetition=r,\n",
    "                        params=params,\n",
    "                        target_metrics=target_metrics,\n",
    "                        shadow_metrics=shadow_metrics,\n",
    "                        mia_metrics=mia_metrics\n",
    "                    )\n",
    "\n",
    "                    results_df = pd.concat([results_df, new_results.to_dataframe()], ignore_index=True)\n",
    "warnings.simplefilter(\"default\")#enable warnings again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df14e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Save the results file\n",
    "results_df.to_csv(results_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-bunch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
