{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salem paper attacks\n",
    "\n",
    "This notebook showcases the three different scenarios presented in Salem *et al.* [ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models](https://arxiv.org/abs/1806.01246).\n",
    "\n",
    "In summary the three presented adversaries are as follows:\n",
    "1. The attacker has access to a dataset drawn from the same distribution as the target data, and uses it to train a single shadow model that infers the membership information.\n",
    "2. The attacker uses a dataset from a different distribution than the target data to do the same as Adv 1.\n",
    "3. The attacker queries the target model and chooses a threshold on the classification probability to determine membership, no shadow models required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "# Classifiers for attack models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import t=privachy version \n",
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the datasets\n",
    "1. We draw data points from a distribution.\n",
    "2. We split these data points into the target dataset and a shadow dataset drawn from the same distribution.\n",
    "3. We also draw a dataset from a different distribution.\n",
    "\n",
    "**NOTE**. ***I make datasets with few samples but with many features to force the target model to overfit.***\n",
    "\n",
    "\n",
    "***NOTE JIM: had to make batch_size 25 so DP optimizer would run with same hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "\n",
    "# (X,y): Original distribution\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_classes=n_classes, \n",
    "                           n_features=300,\n",
    "                           n_informative=300,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           random_state=15\n",
    "                          )\n",
    "# One-hot encoding of the label\n",
    "y = np.eye(n_classes)[y]\n",
    "\n",
    "# (Xt, yt) is the target dataset, owned by the TRE and drawn from the (X,y) distribution\n",
    "# (Xs, ys) is a shadow dataset drawn from the (X,y) distribution\n",
    "Xt, Xs, yt, ys = train_test_split(X, y, test_size=0.50, random_state=15)\n",
    "\n",
    "# (Xd, yd) is a shadow dataset, drawn from a different distribution (different seed)\n",
    "Xd, yd = make_classification(n_samples=1000,\n",
    "                           n_classes=n_classes, \n",
    "                           n_features=300,\n",
    "                           n_informative=300,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           random_state=42\n",
    "                          )\n",
    "yd = np.eye(n_classes)[yd]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the target model\n",
    "\n",
    "*Again, I'm using a rather big model (for the classification task) to favour overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (member) and test (non-member) datasets\n",
    "# Set shuffle to False so that Xt_membership is consistent with Xt, otherwise\n",
    "# we need to stack Xt_member and Xt_nonmember again to get a consistent Xt.\n",
    "Xt_member, Xt_nonmember, yt_member, yt_nonmember = train_test_split(Xt, yt, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Set membership status for future tests\n",
    "Xt_membership = np.vstack(\n",
    "    (\n",
    "        np.ones((Xt_member.shape[0], 1), np.uint8),\n",
    "        np.zeros((Xt_nonmember.shape[0], 1), np.uint8)\n",
    "    )\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 16:32:47.994896: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-12 16:32:47.995212: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Define target model\n",
    "# Tensorflow model (MLP) (making it big to make it overfit)\n",
    "input_data = Input(shape = Xt_member[0].shape)\n",
    "x = Dense(128, activation='relu')(input_data)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Model(input_data, output)\n",
    "target_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size=25\n",
    "epochs = 10\n",
    "\n",
    "# Train target model\n",
    "r = target_model.fit(Xt_member, \n",
    "                     yt_member, \n",
    "                     validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                     epochs=epochs, \n",
    "                     batch_size=batch_size\n",
    "                    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['accuracy'], label='accuracy')\n",
    "plt.plot(r.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUCCESS. The model overfits a lot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So now let's see what we can find out about this model if it is exported just be querying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in particular, as well as all the config details and weights, we can see this was not compiled with the DP version of the optimiser\n",
    " - and if we print target_model.optimizer.__dict__ we get even mor detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'trained model has this type of optimiser {type(target_model.optimizer)}')\n",
    "print( 'and the loss function is {target_model.loss}')\n",
    "print(target_model.optimizer._name)\n",
    "\n",
    "saved_nonDP_optimiser_dict= target_model.optimizer.__dict__\n",
    "\n",
    "test_loss, test_acc = target_model.evaluate(Xs,  ys, verbose=2)\n",
    "print(f'before hacking the model attributes test loss is {test_loss} test accuracy is {test_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so ...\n",
    "We could easily:\n",
    "- write a code that tested the model looking to see if it used for the DP optimiser \n",
    "- make it available via `preliminaryCheck()` and `requestRelease()`\n",
    "\n",
    "But what if a researcher tries to change the optimiser afterwards, but forgets to retrain?\n",
    "\n",
    "**Q** check if malicious user could change min_split_size perior to calling fit() then change back afterwards in sklearn safe version\n",
    "\n",
    "\n",
    "**panicking user follows advice from** https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy\n",
    "\n",
    "- define some params\n",
    "- Define the optimizer and loss function for the learning model. \n",
    "\n",
    "- Compute the loss as a vector of losses per-example rather than as the mean over a minibatch to support gradient manipulation over each training point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###user manually creates DP optimiser and resets model\n",
    "#some params\n",
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 1.3\n",
    "num_microbatches = batch_size\n",
    "learning_rate = 0.25\n",
    "\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "\n",
    "#create the optimiser and loss function\n",
    "\n",
    "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to manually assign these values to the target_model rather than recompile and refit\n",
    "import copy\n",
    "hacked_model = copy.deepcopy(target_model)\n",
    "\n",
    "hacked_model.optimizer = optimizer\n",
    "hacked_model.loss = loss\n",
    "\n",
    "# so what have we got now?\n",
    "\n",
    "print(f'After hacking the model attributes it now looks like the same trained model has this type of optimiser {hacked_model.optimizer}')\n",
    "\n",
    "\n",
    "# can we still use it for prediction even after this hacking?\n",
    "test_loss, test_acc = hacked_model.evaluate(Xs,  ys, verbose=2)\n",
    "print(f'\\n But, importantly, we can still use it to predict: test loss is {test_loss} test accuracy is {test_acc}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### This is what they would have got if they recompiled and reran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#some params\n",
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 1.3\n",
    "num_microbatches = batch_size\n",
    "learning_rate = 0.25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "\n",
    "#create the optimiser and loss function\n",
    "\n",
    "optimizer = tensorflow_privacy.DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "DP_model = Model(input_data, output)\n",
    "DP_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r_DP = DP_model.fit(Xt_member, \n",
    "                     yt_member, \n",
    "                     validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                     epochs=epochs, \n",
    "                     batch_size=batch_size\n",
    "                    )  \n",
    "                    \n",
    "plt.plot(r_DP.history['accuracy'], label='accuracy')\n",
    "plt.plot(r_DP.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()                    \n",
    "                    \n",
    "                    \n",
    "test_loss, test_acc = DP_model.evaluate(Xs,  ys, verbose=2)\n",
    "print(f'\\n The DP trained version gets these test results:  test loss is {test_loss} test accuracy is {test_acc}')\n",
    "\n",
    "\n",
    "print(f'trained model has this type of optimiser {type(DP_model.optimizer)}')\n",
    "print( 'and the loss function is {target_model_DP.loss}')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks for differential privacy based on flags within model optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show that DP was not used\n",
    "def check_DP_used(optimizer):\n",
    "    DPused = False\n",
    "    reason = \"None\"\n",
    "    if ( \"_was_dp_gradients_called\" not in optimizer.__dict__ ):\n",
    "        reason = \"optimiser does not contain key _was_dp_gradients_called so is not DP.\"\n",
    "        DPused = False\n",
    "    elif (optimizer._was_dp_gradients_called==False):\n",
    "        reason= \"although the target model optimiser has been changed to a DP variant, fit() has not been rerun.\"\n",
    "        DPused = False\n",
    "    else:\n",
    "        reason= f\" value of the key: target_model.optimizer._was_dp_gradients_called is {optimizer._was_dp_gradients_called} so DP variant of optimiser has been run\"\n",
    "        DPused=True\n",
    "    return DPused, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what have we learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=====For our original non-DP model=====\")\n",
    "the_optimiser = target_model.optimizer\n",
    "print(f\"The optimiser is of type {type(the_optimiser)}, with name {the_optimiser._name}.\")\n",
    "print(f\"calling get_config() on this optimizer returns {the_optimiser.get_config()}\")\n",
    "value, reason = check_DP_used(the_optimiser)\n",
    "print(f\"Running our check_DP_used function  gives {value} because {reason}\\n\")\n",
    "\n",
    "print(f\"=====For the hacked version of original non-DP model=====\")\n",
    "the_optimiser = hacked_model.optimizer      \n",
    "print(f\"The optimiser is of type {type(the_optimiser)}, with name {the_optimiser._name}.\")\n",
    "print(f\"calling get_config() on this optimizer returns {the_optimiser.get_config()}\")\n",
    "value, reason = check_DP_used(the_optimiser)\n",
    "print(f\"Running our check_DP_used function  gives {value} because {reason}\\n\")\n",
    "\n",
    "print(\"=====For the DP model =======\")\n",
    "the_optimiser= DP_model.optimizer\n",
    "print(f\"The optimiser is of type {type(the_optimiser)}, with name {the_optimiser._name}.\")\n",
    "print(f\"calling get_config() on this optimizer returns {the_optimiser.get_config()}\")\n",
    "value, reason = check_DP_used(the_optimiser)\n",
    "print(f\"Running our check_DP_used function  gives {value} because {reason}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def check_type(key: str, val: Any, cur_val: Any) -> tuple[str, bool]:\n",
    "    \"\"\"Checks the type of a value\"\"\"\n",
    "    print(type(DP_model.optimizer))\n",
    "    if isinstance(cur_val, val) == False:\n",
    "        disclosive = True\n",
    "        msg = (\n",
    "            f\"- parameter {key} = {cur_val}\"\n",
    "            f\" identified as different type than the recommended fixed value of {val}.\"\n",
    "        )\n",
    "    else:\n",
    "        disclosive = False\n",
    "        msg = \"\"\n",
    "    return msg, disclosive\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "check_type(key=\"optimizer\", val=tensorflow_privacy.DPKerasSGDOptimizer,cur_val = DP_model.optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so the test accuracy is now significantly reduced if they properly recompile and retrain using the DP code\n",
    "\n",
    "### and what is the nominal privacy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=Xt.shape[0],\n",
    "                                              batch_size=batch_size,\n",
    "                                              noise_multiplier=noise_multiplier,\n",
    "                                              epochs=epochs,\n",
    "                                              delta=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try the SafeKerasModel version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import safemodel\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(safemodel.safemodel)\n",
    "importlib.reload(safemodel.classifiers.safekeras)\n",
    "from safemodel.classifiers.safekeras import Safe_KerasModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args is a <class 'tuple'> = ()  kwargs is a <class 'dict'>= {'inputs': <KerasTensor: shape=(None, 300) dtype=float32 (created by layer 'input_1')>, 'outputs': <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'dense_3')>}\n",
      "WARNING: model parameters may present a disclosure risk:\n",
      "- parameter optimizer = None identified as different type to recommendation of tensorflow_privacy.privacy.optimizers.dp_optimizer_keras.make_keras_optimizer_class.<locals>.DPOptimizerClass.Nothing currently implemented to change type of parameter optimizer from NoneType to tensorflow_privacy.privacy.optimizers.dp_optimizer_keras.make_keras_optimizer_class.<locals>.DPOptimizerClass.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer=None\n",
    "safeModel = Safe_KerasModel(inputs= input_data, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safeModel.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow_privacy.privacy.optimizers.dp_optimizer_keras.make_keras_optimizer_class.<locals>.DPOptimizerClass object at 0x28e367190>\n"
     ]
    }
   ],
   "source": [
    "safeModel.compile()#optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(safeModel.optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimiser is type <class 'tensorflow_privacy.privacy.optimizers.dp_optimizer_keras.make_keras_optimizer_class.<locals>.DPOptimizerClass'>\n",
      "<class 'module'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 2 must be a class or tuple of classes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4m/prf7hx2x71dd2kz4x0g_v6hc000369/T/ipykernel_52850/3476543217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(tensorflow_privacy.privacy.optimizers.dp_optimizer_keras)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' {issubclass(theType,tensorflow_privacy.privacy.optimizers.dp_optimizer_keras)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 2 must be a class or tuple of classes"
     ]
    }
   ],
   "source": [
    "theType= type(safeModel.optimizer)\n",
    "print(f'optimiser is type {theType}')\n",
    "\n",
    "print (f'{type(tensorflow_privacy.privacy.optimizers.dp_optimizer_keras)}')\n",
    "print(f' {issubclass(theType,tensorflow_privacy.privacy.optimizers.dp_optimizer_keras)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 1.3\n",
    "num_microbatches = batch_size\n",
    "learning_rate = 0.25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "\n",
    "#create the optimiser and loss function\n",
    "\n",
    "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.fit(Xt_member, \n",
    "              yt_member, \n",
    "              validation_data=(Xt_nonmember, yt_nonmember),\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.save('safekeras.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.preliminary_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.request_release('safekeras.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit using recommended params\n",
    "print(\"***Test 1: researcher doesn't change recommended params\")\n",
    "safeKerasModel1 = Safe_KerasModel(input_data, output)\n",
    "safeKerasModel1.compile()\n",
    "safeKerasModel1.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel1.save(name=\"safe1.pkl\")\n",
    "safeKerasModel1.preliminary_check()\n",
    "safeKerasModel1.request_release(filename=\"safe1.pkl\")\n",
    "\n",
    "\n",
    "# change model params to recommended values\n",
    "print(\"\\n***Test 2: researcher changes params safely\")\n",
    "safeKerasModel2 = Safe_KerasModel(input_data, output)\n",
    "safeKerasModel2.compile()\n",
    "safeKerasModel2.optimizer=\"DPAdam\"\n",
    "safeKerasModel2.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel2.save(name=\"safe2.pkl\")\n",
    "safeKerasModel2.preliminary_check()\n",
    "safeKerasModel2.request_release(filename=\"safe2.pkl\")\n",
    "\n",
    "# change one model params in an unsafe way\n",
    "print(\"\\n***Test 3: researcher changes string params unsafely\")\n",
    "safeKerasModel3 = Safe_KerasModel(input_data, output)\n",
    "safeKerasModel3.compile()\n",
    "safeKerasModel3.optimizer=\"Adam\"\n",
    "safeKerasModel3.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel3.save(name=\"unsafe3.pkl\")\n",
    "safeKerasModel3.preliminary_check()\n",
    "safeKerasModel3.request_release(filename=\"unsafe3.pkl\")\n",
    "\n",
    "# change another model params in an  unsafe way\n",
    "\n",
    "\n",
    "# change another model params in an  unsafe way\n",
    "print(\"\\n***Test 5: researcher changes string and numeric params unsafely\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change another model params in an  unsafe way but tells preliminary_check() not to overwrite params\n",
    "print(\"\\n***Test 6: researcher changes string and numeric params unsafely\")\n",
    "safeKerasModel6 = Safe_KerasModel(input_data, output)\n",
    "safeKerasModel6.compile()\n",
    "safeKerasModel6.optimizer=\"Adam\"\n",
    "safeKerasModel6.fit(Xt_member, \n",
    "                    yt_member, \n",
    "                    validation_data=(Xt_nonmember, yt_nonmember),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size)\n",
    "safeKerasModel6.save(name=\"unsafe6.pkl\")\n",
    "safeKerasModel6.preliminary_check(apply_constraints=False)\n",
    "safeKerasModel6.request_release(filename=\"unsafe6.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
